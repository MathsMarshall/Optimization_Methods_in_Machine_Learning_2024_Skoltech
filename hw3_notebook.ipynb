{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QucZcYA5iP-T"
      },
      "source": [
        "# Optimization Methods in Machine Learning\n",
        "\n",
        "## Homework 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2898zmkeiP-X"
      },
      "source": [
        "For code-writing part of homework, please submit the single Jupyter Notebook file, where only Python and Markdown/LaTeX are used. The submission should be in the following format: *YourName_HW3.ipynb*.\n",
        "\n",
        "You are free to modify the function templates and use additional libraries. However, do not use built-in functions if the assignment requires you to implement the method from scratch. Do not forget to add necessary explanations and comments.\n",
        "\n",
        "\n",
        "The works will be checked for plagiarism. The score will be divided by the number of similar works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv2W2nmFiP-Y"
      },
      "source": [
        "### Problem 1 (8 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Consider the empirical risk minimization problem with reguralizer:\n",
        "\n",
        "\\begin{equation}\n",
        "\\min_{w \\in \\mathbb{R}^d} \\frac{1}{n} \\sum\\limits_{i=1}^n \\ell (g(w, x_i), y_i) + \\frac{\\lambda}{2} \\| w\\|^2_2,\n",
        "\\end{equation}\n",
        "\n",
        "where $\\ell: \\mathbb{R} \\times \\mathbb{R} \\to \\mathbb{R}$ is a loss function, $g : \\mathbb{R}^d \\times \\mathbb{R}^x \\to \\mathbb{R}$ is a model, $w$ are parameters of the model, $\\{x_i, y_i\\}_{i=1}^n$ is data of objects $x_i \\in \\mathbb{R}^x$ and labels $y_i \\in \\mathbb{R}$.\n",
        "\n",
        "We use the linear model $g(w, x) = w^T x$ and the logistic/sigmoid loss function: $\\ell(z,y) = \\ln (1 + \\exp(-yz))$ ($y$ must take only 2 values $0$ or $1$).\n",
        "As we already know, the resulting problem is called a logistic regression problem.\n",
        "\n",
        "This problem can be rewritten as follows:\n",
        "$$\n",
        "\\min_{w \\in \\mathbb{R}^d} f(w) := \\frac{1}{s} \\sum\\limits_{j=1}^s f_j(w) := \\frac{1}{s} \\sum\\limits_{j=1}^s \\left[\\frac{1}{b} \\sum\\limits_{i=1}^b l (g(w, x_{(j-1)b + i}), y_{(j-1)b + i}) + \\frac{\\lambda}{2} \\| w\\|^2_2\\right],\n",
        "$$\n",
        "where $b$ is the batch size, $s$ is the number of batches, and $b s = n$ is the total sample size.\n",
        "\n",
        "The gradient of $f_j$:\n",
        "$$\n",
        "\\nabla f_j(w) = \\frac{1}{b} \\sum_{i=1}^b \\frac{-y_{(j-1)b + i} x_{(j-1)b + i}}{1 + \\exp(y_{(j-1)b + i} w^Tx_{(j-1)b + i})}.\n",
        "$$\n",
        "The Lipschitz constant of the gradient $\\nabla f_j$ can be estimated as $L_j = \\frac{1}{4b} \\sum\\limits_{i=1}^b \\| x_{(j-1)b + i} \\|^2_2$.\n",
        "\n",
        "We will work with [mushrooms dataset](https://www.kaggle.com/datasets/prishasawhney/mushroom-dataset/data).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install kagglehub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "\n",
        "path = kagglehub.dataset_download(\"prishasawhney/mushroom-dataset\")\n",
        "\n",
        "data = pd.read_csv(f\"{path}/mushroom_cleaned.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(1 pts) Implement the ability to uniformly divide the training part of the dataset into batches of size $b$ ($b$ is a parameter)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(1 pts) Implement the SGD method:\n",
        "$$\n",
        "w^{k+1} = w^k - \\gamma_k \\nabla f_{j_k} (w^k),\n",
        "$$\n",
        "where the number $j_k$ is generated independently and uniformly from $\\{1, \\ldots, s \\}$. For the tasks below, you may need to be able to measure the running time of the method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(1 pts) Solve the optimization problem on the training sample using the implemented method. Take $b = 10$, and the step is $\\gamma_k \\equiv \\frac{1}{L}$. Draw the convergence plot: the value of the convergence criterion (e.g. $\\frac{\\| \\nabla f(w^k)\\|}{\\| \\nabla f(w^0)\\|}$) from the iteration number. Make a conclusion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(2 pts) Vary the batch size: $b = 1, 10, 100, 1000$, and take the step size equal to $\\gamma_k \\equiv \\frac{1}{L}$. Draw the convergence plot: the value of the convergence criterion from the iteration number for each $b$. Does this plot reflect a fair comparison? Why? Figure out how to compare the results to each other more honestly and draw a new comparison plot. Make a conclusion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(2 pts) Let us fix $b = 10$ and try to change the strategy of choosing the step:\n",
        "\n",
        "1) $\\gamma_k \\equiv \\frac{1}{L}$ as we did before,\n",
        "\n",
        "2) $\\gamma_k = \\frac{1}{\\sqrt{k + 1}}$, \n",
        "\n",
        "3) $\\gamma_k = \\frac{1}{k + 1}$.\n",
        "\n",
        "Draw the convergence plot: the value of the convergence criterion from the iteration number. Make a conclusion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(1 pts) Look at the accuracy of the model trained with SGD. Repeat point d)-e), but now plot the accuracy dependence, not the convergence criterion. Make a conclusion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#your code"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
